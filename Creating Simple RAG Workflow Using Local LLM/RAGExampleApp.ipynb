{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.workflow import Event, Context, Workflow, StartEvent, StopEvent, step\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "    query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    def __init__(self, model_name=\"granite4:350m\", embedding_model=\"BAAI/bge-small-en-v1.5\"):\n",
    "        super().__init__()\n",
    "        # Initialize LLM and embedding model\n",
    "        self.llm = Ollama(model=model_name)\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embedding_model)\n",
    "\n",
    "        # Configure global settings\n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = self.embed_model\n",
    "\n",
    "        self.index = None\n",
    "\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Entry point to ingest documents from a directory.\"\"\"\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        self.index = VectorStoreIndex.from_documents(documents=documents)\n",
    "        return StopEvent(result=self.index)\n",
    "\n",
    "    @step\n",
    "    async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent | None:\n",
    "        \"\"\"Entry point for RAG retrieval.\"\"\"\n",
    "        query = ev.get(\"query\")\n",
    "        index = ev.get(\"index\") or self.index\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever(similarity_top_k=2)\n",
    "        nodes = await retriever.aretrieve(query)\n",
    "        return RetrieverEvent(nodes=nodes, query=query)\n",
    "\n",
    "    @step\n",
    "    async def synthesize(self, ctx: Context, ev: RetrieverEvent) -> StopEvent:\n",
    "        \"\"\"Generate a response using retrieved nodes.\"\"\"\n",
    "        summarizer = CompactAndRefine(streaming=True, verbose=True)\n",
    "        query = ev.query\n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(result=response)\n",
    "\n",
    "    async def query(self, query_text: str):\n",
    "        \"\"\"Helper method to perform a complete RAG query.\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No documents have been ingested. Call ingest_documents first.\")\n",
    "\n",
    "        result = await self.run(query=query_text, index=self.index)\n",
    "        return result\n",
    "\n",
    "    async def ingest_documents(self, directory: str):\n",
    "        \"\"\"Helper method to ingest documents.\"\"\"\n",
    "        result = await self.run(dirname=directory)\n",
    "        self.index = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entrypoint is ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 22:43:05,418 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####---> Started RAG WorkFlow..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 22:43:09,565 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####---> Started Ingesting Documents..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x17fabffcf50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the workflow\n",
    "print(\"#####---> Started RAG WorkFlow..\")\n",
    "w = RAGWorkflow()\n",
    "\n",
    "# Ingest documents\n",
    "print(\"#####---> Started Ingesting Documents..\")\n",
    "await w.ingest_documents(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second entry point is retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####---> Started Querying..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 22:43:16,167 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####---> Started Printing Response..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 22:43:16,958 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, Sultan Mehmed II became the leader of the Ottoman Empire in the year 1453."
     ]
    }
   ],
   "source": [
    "# Perform a query\n",
    "print(\"#####---> Started Querying..\")\n",
    "result = await w.query(\"How old was Sultan Mehmed when he became the leader of the Ottoman Empire? What was his age?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"#####---> Started Printing Response..\")\n",
    "async for chunk in result.async_response_gen():\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
