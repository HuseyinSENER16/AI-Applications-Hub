# ğŸš€ Creating a Simple RAG Workflow Using Local LLM

This project demonstrates how to build a **Retrieval-Augmented Generation (RAG)** workflow using **LlamaIndex** and local LLMs. It allows you to chat with your private documents entirely offline, ensuring data privacy and reducing latency.

---

## ğŸ“– Overview

**RAG (Retrieval-Augmented Generation)** is a powerful technique that enhances LLM capabilities by retrieving relevant information from a specific dataset before generating a response. This ensures the model's output is grounded in factual, provided data rather than just its training knowledge.

### Why LlamaIndex Workflows?
This project utilizes the new **LlamaIndex Workflows** API, which provides:
- **Event-Driven Architecture**: Each step in the RAG process (Ingestion, Retrieval, Synthesis) is triggered by events.
- **Modularity**: Easily swap out components (e.g., use a different retriever or LLM).
- **Asynchronous Execution**: Designed for high-performance, non-blocking operations.

---

## ğŸ› ï¸ Tech Stack

- ğŸ¦™ **LlamaIndex**: Data framework for LLM applications.
- ğŸ“¦ **Ollama**: Local LLM runner.
- ğŸ¤— **HuggingFace**: Embedding models (`bge-small-en-v1.5`).
- ğŸ **Python**: Core programming language.
- ğŸ““ **Jupyter Notebook**: Interactive exploration.

---

## ğŸ“‚ Project Structure

```text
Creating Simple RAG Workflow Using Local LLM/
â”œâ”€â”€ data/                    # ğŸ“ Your documents go here
â”‚   â””â”€â”€ History_Moment.txt   # Example text file
â”œâ”€â”€ RAGExampleApp.py         # ğŸ Core workflow implementation
â”œâ”€â”€ RAGExampleApp.ipynb      # ğŸ““ Interactive notebook version
â”œâ”€â”€ requirements.txt         # ğŸ“‹ Python dependencies
â”œâ”€â”€ env.example              # âš™ï¸ Environment variable template
â””â”€â”€ README.md                # ğŸ“– Project documentation
```

---

## ğŸš€ Getting Started

### 1. Prerequisites
- Install [Ollama](https://ollama.ai/) and pull the required model:
  ```bash
  ollama pull granite4:350m
  ```

### 2. Installation
Clone the repository and install the dependencies:
```bash
pip install -r requirements.txt
```

### 3. Usage

#### Run the Python Script:
```bash
python RAGExampleApp.py
```

#### Steps Involved:
1.  **Ingestion** ğŸ“¥: The script reads document(s) from the `data/` folder.
2.  **Indexing** ğŸ—‚ï¸: It converts the text into vector embeddings and stores them in a Vector Store Index.
3.  **Retrieval** ğŸ”: When a query is made, it finds the most relevant document chunks.
4.  **Synthesis** âœï¸: The LLM generates a response based on the retrieved context.

---

## ğŸ’¡ Example

**Query:** *"How old was Sultan Mehmed when he became the leader of the Ottoman Empire?"*

**RAG Process:**
1.  Searches `History_Moment.txt` for "Sultan Mehmed leader age".
2.  Retrieves the relevant paragraph.
3.  Synthesizes the answer: *"Sultan Mehmed II was 12 years old when he first ascended to the throne in 1444..."*

---

## ğŸ”§ Workflow Customization

You can change the models easily in `RAGExampleApp.py`:

```python
# Customizing the models
workflow = RAGWorkflow(
    model_name="llama3", 
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)
```

---

## ğŸ” Privacy
Since everything runs locally via Ollama and HuggingFace, your data **never leaves your machine**. 

---
*Created with â¤ï¸ for AI Application Hub.*
