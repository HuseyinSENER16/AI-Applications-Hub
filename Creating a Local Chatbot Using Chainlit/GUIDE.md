# ğŸ¤– Building a 100% Local Chatbot with Chainlit and Ollama

Welcome to a comprehensive guide on creating a fully local chatbot that runs entirely on your machine without any external dependencies. This implementation uses Chainlit for the UI and Ollama for local LLM processing.

## ğŸ“‹ Table of Contents
- [âœ¨ Features](#features)
- [ğŸ”§ Prerequisites](#prerequisites)
- [ğŸš€ Getting Started](#getting-started)
- [ğŸ“‹ Code Structure](#code-structure)
- [ğŸ’» Detailed Code Explanation](#detailed-code-explanation)
- [ğŸ“¸ Screenshots](#screenshots)
- [ğŸ”„ How It Works](#how-it-works)
- [ğŸ’¡ Customization Tips](#customization-tips)
- [ğŸš€ Future Enhancements](#future-enhancements)

## âœ¨ Features

- ğŸ  **100% Local Processing**: No external APIs or internet required after setup
- ğŸ¨ **Beautiful UI**: Chainlit provides an elegant chat interface
- ğŸ‘ï¸ **Vision Support**: Can process images using Llama3.2-Vision model
- ğŸ“ **Streaming Responses**: Real-time token streaming for natural interaction
- ğŸ“ **File Upload**: Supports image uploads for multimodal interactions
- ğŸ§  **Conversation Memory**: Maintains context throughout the chat session

## ğŸ”§ Prerequisites

Before running the application, ensure you have:

- ğŸ Python 3.8+ installed
- ğŸ¦™ [Ollama](https://ollama.ai/) installed and running
- ğŸ§  Downloaded a compatible model (e.g., `granite4:350m` as used in the code)
- ğŸ§© Required Python packages: `chainlit`, `ollama`

Install the required packages:
```bash
pip install chainlit ollama
```

Pull the required model:
```bash
ollama pull granite4:350m
```

## ğŸš€ Getting Started

1. Clone or download the repository
2. Install dependencies: `pip install -r requirements.txt`
3. Ensure Ollama is running: `ollama serve`
4. Pull the model: `ollama pull granite4:350m`
5. Run the application: `chainlit run Test.py`
6. Open your browser to the provided URL (usually http://localhost:8000)

## ğŸ“‹ Code Structure

```text
Test.py
â”œâ”€â”€ Import Statements
â”œâ”€â”€ Chat Session Initialization
â”‚   â””â”€â”€ System Role Setup
â”œâ”€â”€ Tool Function
â”‚   â”œâ”€â”€ Message History Management
â”‚   â”œâ”€â”€ Image Processing
â”‚   â””â”€â”€ Ollama Interaction
â”œâ”€â”€ Message Handler
â”‚   â”œâ”€â”€ Image Detection
â”‚   â”œâ”€â”€ Response Streaming
â”‚   â””â”€â”€ User Interaction
â””â”€â”€ Execution Flow
```

## ğŸ’» Detailed Code Explanation

### 1. ğŸ“¦ **Import Statements**

```python
import chainlit as cl
import ollama
```

- `chainlit` provides the UI framework for our chatbot
- `ollama` enables interaction with local LLMs

### 2. ğŸ¯ **Chat Session Initialization**

```python
@cl.on_chat_start
async def start_chat():
    cl.user_session.set(
        "interaction",
        [
            {
                "role": "system",
                "content": "You are a helpful assistant.",
            }
        ],
    )

    msg = cl.Message(content="")

    start_message = "Hello, I'm your 100% local alternative to ChatGPT running on Llama3.2-Vision. How can I help you today?"

    for token in start_message:
        await msg.stream_token(token)

    await msg.send()
```

This function runs when a new chat session begins:

- âœ… **Session Setup**: Creates an initial conversation history with a system prompt
- âœ… **Welcome Message**: Creates a streaming welcome message to the user
- âœ… **User Experience**: The message appears character by character for a natural feel

### 3. âš™ï¸ **Tool Function**

```python
@cl.step(type="tool")
async def tool(input_message, image=None):

    interaction = cl.user_session.get("interaction")

    if image:
        interaction.append({"role": "user",
                            "content": input_message,
                            "images": image})
    else:
        interaction.append({"role": "user",
                            "content": input_message})

    response = ollama.chat(model="granite4:350m",
                           messages=interaction)

    interaction.append({"role": "assistant",
                        "content": response.message.content})

    return response
```

This function handles the core interaction logic:

- ğŸ” **History Management**: Retrieves the current conversation history
- ğŸ–¼ï¸ **Multimodal Support**: Adds user message with or without images
- ğŸ§  **LLM Processing**: Calls the local Ollama model with the conversation history
- ğŸ“¥ **Response Storage**: Saves the AI's response to the conversation history
- ğŸ“¤ **Return**: Returns the model's response for further processing

### 4. ğŸ“¨ **Message Handler**

```python
@cl.on_message
async def main(message: cl.Message):

    images = [file for file in message.elements if "image" in file.mime]

    if images:
        tool_res = await tool(message.content, [i.path for i in images])

    else:
        tool_res = await tool(message.content)

    msg = cl.Message(content="")

    for token in tool_res.message.content:
        await msg.stream_token(token)

    await msg.send()
```

This function processes each user message:

- ğŸ–¼ï¸ **Image Detection**: Checks if the message contains images
- ğŸ”§ **Tool Execution**: Calls the tool function with or without images
- ğŸ’¬ **Response Streaming**: Streams the AI's response token by token
- ğŸ“¤ **Message Delivery**: Sends the complete response to the user

## ğŸ“¸ Screenshots

### ğŸ–¥ï¸ Local Chatbot Interface
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Local Chatbot Interface             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  ğŸ’¬ Hello, I'm your 100% local          â”‚
â”‚      alternative to ChatGPT running     â”‚
â”‚      on Llama3.2-Vision. How can       â”‚
â”‚      I help you today?                 â”‚
â”‚                                         â”‚
â”‚  [Type your message here...]           â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ How It Works

1. ğŸš€ **Initialization**: When the chat starts, a system message is set up
2. ğŸ‰ **Welcome**: A streaming welcome message is displayed to the user
3. ğŸ“¨ **User Input**: When a user sends a message:
   - The system checks for images in the message
   - The message (with or without images) is sent to the Ollama model
   - Conversation history is maintained for context
4. ğŸ¤– **AI Response**: The local model processes the input and generates a response
5. ğŸ“¤ **Delivery**: The response is streamed token by token to the user
6. ğŸ”„ **Loop**: The process repeats for the next message

## ğŸ’¡ Customization Tips

### ğŸ§  **Changing Models**
To use a different model, replace `"granite4:350m"` with your preferred model:
```python
response = ollama.chat(model="your-preferred-model", messages=interaction)
```

### ğŸ¨ **Customizing System Prompt**
Modify the system message in the `start_chat()` function:
```python
{
    "role": "system",
    "content": "You are a custom helpful assistant with specific capabilities.",
}
```

### ğŸ–¼ï¸ **Enhanced Vision Processing**
Add image preprocessing capabilities by extending the image handling in the tool function.

## ğŸš€ Future Enhancements

- ğŸ“„ **Document Processing**: Add support for PDF, DOCX and other document formats
- ğŸ¯ **Fine-tuning**: Implement custom fine-tuning capabilities
- ğŸ” **Privacy Controls**: Add enhanced privacy and data handling options
- ğŸŒ **Multi-language Support**: Add support for multiple languages
- âš¡ **Performance Optimization**: Implement caching and optimization techniques

## ğŸ“š Resources

- [Chainlit Documentation](https://docs.chainlit.io/)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Local LLM Models](https://ollama.ai/library)

## ğŸ¤ Contributing

We welcome contributions to improve this local chatbot implementation! Feel free to submit issues or pull requests to enhance functionality.

## ğŸ“ License

This project is open source and available under the MIT License.